{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis\n",
    "\n",
    "This notebook briefly shows some ways and techniques for analysis and comparison of trained models from the pipeline.\n",
    "\n",
    "> **Note**: at time of this writing, in order to get the desired output this notebook needs to be run in the traditional fashion of Juputer notebooks, i.e.:\n",
    "```shell\n",
    "# In your terminal, execute:\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "# Then launch this file\n",
    "```\n",
    "\n",
    "## Install Jupyter extensions\n",
    "\n",
    "> **Note**: If running in a local Jupyter notebook, then these Jupyter extensions must be installed in the environment before running Jupyter. You need to check which version of Jupyterlab you're running as well as the version of TFMA.\n",
    "\n",
    "```shell\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "jupyter nbextension enable --py tensorflow_model_analysis\n",
    "jupyter labextension install tensorflow_model_analysis@0.36.0\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager@3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to execute the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "# !jupyter nbextension enable --py tensorflow_model_analysis\n",
    "# !jupyter labextension install tensorflow_model_analysis@0.36.0\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager@3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension list\n",
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.types import standard_artifacts\n",
    "\n",
    "# import witwidget\n",
    "# from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "\n",
    "from utils.mlmd_helpers import get_latest_artifacts\n",
    "\n",
    "from pipeline.configs import PIPELINE_NAME\n",
    "\n",
    "print(f'TF version: {tf.version.VERSION}')\n",
    "print(f'TFMA version: {tfma.version.VERSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata artifacts\n",
    "\n",
    "In order to investigate generated components from the pipeline we need to fetch the desired artifacts.  \n",
    "\n",
    "We start by fetching the artifacts (if generated) from `Tuner`, `Trainer`, and `Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read artifact information from metadata store.\n",
    "\n",
    "# Metadata store path\n",
    "METADATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..',\n",
    "                                             'outputs/tfx_metadata',\n",
    "                                             PIPELINE_NAME,\n",
    "                                             'metadata.db'))\n",
    "\n",
    "# Metadata store config\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "              METADATA_PATH)\n",
    "\n",
    "with metadata.Metadata(metadata_connection_config) as store:\n",
    "    # Load Tuner artifacts\n",
    "    try:\n",
    "        tuner_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Tuner')\n",
    "    except AttributeError:\n",
    "        print('Tuner not available')\n",
    "        \n",
    "    # Load Model artifacts\n",
    "    try:\n",
    "        model_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Trainer')\n",
    "    except AttributeError:\n",
    "        print('Trainer not available')\n",
    "    \n",
    "    # Load Evaluator artifacts\n",
    "    try:\n",
    "        model_eval_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Evaluator')\n",
    "    except AttributeError:\n",
    "        print('Evaluator not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the URI's of the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure output paths\n",
    "\n",
    "# Exact paths to output artifacts can be found in the execution logs\n",
    "# or KFP Web UI if you are using kubeflow.\n",
    "\n",
    "try:\n",
    "    tuner_run_path = os.path.abspath(os.path.join('..', tuner_artifacts['best_hyperparameters'][-1].uri))\n",
    "    tuner_run_id = tuner_artifacts['best_hyperparameters'][-1].id\n",
    "    print(f'Generated tuner result: {tuner_run_path}')\n",
    "    print(f'Tuner execution id: {tuner_run_id}')\n",
    "except NameError:\n",
    "    print('Tuner not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_run_path = os.path.abspath(os.path.join('..', model_artifacts['model_run'][-1].uri))\n",
    "    model_run_id = model_artifacts['model_run'][-1].id\n",
    "    print(f'Generated model result: {model_run_path}')\n",
    "    print(f'Model execution id: {model_run_id}')\n",
    "except NameError:\n",
    "    print('Model not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_blessed_path = os.path.abspath(os.path.join('..', model_eval_artifacts['blessing'][-1].uri))\n",
    "    model_blessed_id = model_eval_artifacts['blessing'][-1].id\n",
    "    print(f'Generated model blessing result: {model_blessed_path}')\n",
    "    print(f'Blessing execution id: {model_blessed_id}')\n",
    "except NameError:\n",
    "    print('Model blessing not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_eval_path = os.path.abspath(os.path.join('..', model_eval_artifacts['evaluation'][-1].uri))\n",
    "    model_eval_id = model_eval_artifacts['evaluation'][-1].id\n",
    "    print(f'Generated model evaluation result: {model_eval_path}')\n",
    "    print(f'Evaluator execution id: {model_eval_id}')\n",
    "except NameError:\n",
    "    print('Model evaluation not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training investigation\n",
    "\n",
    "The `model_run` output acts as the working directory and can be used to output non-model related output (e.g., TensorBoard logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model results to Tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {model_run_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The `blessing` output simply states if the candidate model was blessed. The artifact URI will have a `BLESSED` or `NOT_BLESSED` file depending on the result. As mentioned earlier, this first run will pass the evaluation because there is no baseline model yet.  \n",
    "\n",
    "The `evaluation` output, on the other hand, contains the evaluation logs and can be used to visualize the global metrics on the entire evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = tfma.load_eval_result(model_eval_path)\n",
    "\n",
    "print(tfma.load_validation_result(model_eval_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Metrics\n",
    "\n",
    "You can view the metrics with the [`tfma.view.render_slicing_metrics()`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_slicing_metrics) method. By default, the views will display the `Overall` slice. To view a particular slice you can pass in a feature name to the `slicing_column` argument as shown below. You can visualize the different metrics through the `Show` dropdown menu and you can hover over the bar charts to show the exact value measured. \n",
    "\n",
    "We encourage you to try the different options you see and also modify the command. Here are some examples:\n",
    "\n",
    "* Removing the `slicing_column` argument will produce the overall slice.\n",
    "* You can also pass in `race` (since it was specified in the eval config) to see the results for that particular slice.\n",
    "* Using the `Examples (Weighted) Threshold` slider above 5421 will remove the `Female` slice because it has less examples than that.\n",
    "* Toggling the `View` dropdown to `Metrics Histogram` will show the results divided into buckets. For example, if you're slicing column is `sex` and the `Histogram Type` dropdown is at `Slice Counts`, then you will one slice in two of the 10 (default) buckets since we only have two values for that feature ('Male' and 'Female'). The x-axis show the values for the metric in the `Select Metric` dropdown. This is the default view when the number of slices is large.\n",
    "* At the bottom of the screen, you will notice the measurements also presented in tabular format. You can sort it by clicking on the feature name headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render overall results\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Slice on column\n",
    "tfma.view.render_slicing_metrics(eval_result,\n",
    "                                 slicing_column='trip_start_day')\n",
    "                                 # slicing_spec=tfma.slicer.SingleSliceSpec(columns=['trip_start_hour']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Render plots\n",
    "tfma.view.render_plot(\n",
    "    eval_result,\n",
    "    tfma.SlicingSpec(feature_values={'trip_start_month': '1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the What-if Tool to interpret your model\n",
    "\n",
    "Once your model has deployed, you're ready to connect it to the What-if Tool using the `WitWidget`.  \n",
    ">**Note**: You can ignore the message `TypeError(unsupported operand type(s) for -: 'int' and 'list')` while creating a What-if Tool visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format a subset of the test data to send to the What-if Tool for visualization\n",
    "# Append ground truth label value to training data\n",
    "\n",
    "# This is the number of examples you want to display in the What-if Tool\n",
    "# num_wit_examples = 500\n",
    "# test_examples = np.hstack((x_test[:num_wit_examples].values,y_test[:num_wit_examples].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a What-if Tool visualization, it may take a minute to load\n",
    "# See the cell below this for exploration ideas\n",
    "\n",
    "# # This prediction adjustment function is needed as this xgboost model's\n",
    "# # prediction returns just a score for the positive class of the binary\n",
    "# # classification, whereas the What-If Tool expects a list of scores for each\n",
    "# # class (in this case, both the negative class and the positive class).\n",
    "# def adjust_prediction(pred):\n",
    "#   return [1 - pred, pred]\n",
    "# \n",
    "# config_builder = (WitConfigBuilder(test_examples.tolist(), data.columns.tolist() + ['mortgage_status'])\n",
    "#   .set_ai_platform_model(GCP_PROJECT, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n",
    "#   .set_target_feature('mortgage_status')\n",
    "#   .set_label_vocab(['denied', 'approved']))\n",
    "# WitWidget(config_builder, height=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
