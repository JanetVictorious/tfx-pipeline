{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis\n",
    "\n",
    "This notebook briefly shows some ways and techniques for analysis and comparison of trained models from the pipeline.\n",
    "\n",
    "> **Note**: at time of this writing, in order to get the desired output this notebook needs to be run in the traditional fashion of Juputer notebooks, i.e.:\n",
    "```shell\n",
    "# In your terminal, execute:\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "# Then launch this file\n",
    "```\n",
    "\n",
    "## Install Jupyter extensions\n",
    "\n",
    "> **Note**: If running in a local Jupyter notebook, then these Jupyter extensions must be installed in the environment before running Jupyter. You need to check which version of Jupyterlab you're running as well as the version of TFMA.\n",
    "\n",
    "```shell\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "jupyter nbextension enable --py tensorflow_model_analysis\n",
    "jupyter labextension install tensorflow_model_analysis@0.36.0\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager@3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to execute the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter nbextension install --py --symlink tensorflow_model_analysis\n",
    "# !jupyter nbextension enable --py tensorflow_model_analysis\n",
    "# !jupyter labextension install tensorflow_model_analysis@0.36.0\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager@3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known nbextensions:\n",
      "  config dir: /Users/viktor.eriksson2/.jupyter/nbconfig\n",
      "    notebook section\n",
      "      jupyter-js-widgets/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "      tensorflow_model_analysis/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "  config dir: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/.venv/etc/jupyter/nbconfig\n",
      "    notebook section\n",
      "      jupyter-js-widgets/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "      wit-widget/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "JupyterLab v3.2.9\n",
      "/Users/viktor.eriksson2/Documents/github/tfx-pipeline/.venv/share/jupyter/labextensions\n",
      "        @jupyter-widgets/jupyterlab-manager v3.0.1 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_widgets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension list\n",
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.7.0\n",
      "TFMA version: 0.36.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libs\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.types import standard_artifacts\n",
    "\n",
    "import witwidget\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "\n",
    "from utils.mlmd_helpers import get_latest_artifacts\n",
    "\n",
    "from pipeline.configs import PIPELINE_NAME\n",
    "\n",
    "print(f'TF version: {tf.version.VERSION}')\n",
    "print(f'TFMA version: {tfma.version.VERSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata artifacts\n",
    "\n",
    "In order to investigate generated components from the pipeline we need to fetch the desired artifacts.  \n",
    "\n",
    "We start by fetching the artifacts (if generated) from `Tuner`, `Trainer`, and `Evaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuner not available\n"
     ]
    }
   ],
   "source": [
    "# Read artifact information from metadata store.\n",
    "\n",
    "# Metadata store path\n",
    "METADATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..',\n",
    "                                             'outputs/tfx_metadata',\n",
    "                                             PIPELINE_NAME,\n",
    "                                             'metadata.db'))\n",
    "\n",
    "# Metadata store config\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "              METADATA_PATH)\n",
    "\n",
    "with metadata.Metadata(metadata_connection_config) as store:\n",
    "    # Load Tuner artifacts\n",
    "    try:\n",
    "        tuner_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Tuner')\n",
    "    except AttributeError:\n",
    "        print('Tuner not available')\n",
    "        \n",
    "    # Load Model artifacts\n",
    "    try:\n",
    "        model_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Trainer')\n",
    "    except AttributeError:\n",
    "        print('Trainer not available')\n",
    "    \n",
    "    # Load Evaluator artifacts\n",
    "    try:\n",
    "        model_eval_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Evaluator')\n",
    "    except AttributeError:\n",
    "        print('Evaluator not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the URI's of the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuner not defined\n",
      "----------\n",
      "Generated model result: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Trainer/model_run/7\n",
      "Model execution id: 14\n",
      "----------\n",
      "Generated model blessing result: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Evaluator/blessing/8\n",
      "Blessing execution id: 15\n",
      "----------\n",
      "Generated model evaluation result: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Evaluator/evaluation/8\n",
      "Evaluator execution id: 16\n"
     ]
    }
   ],
   "source": [
    "# Configure output paths\n",
    "\n",
    "# Exact paths to output artifacts can be found in the execution logs\n",
    "# or KFP Web UI if you are using kubeflow.\n",
    "\n",
    "try:\n",
    "    tuner_run_path = os.path.abspath(os.path.join('..', tuner_artifacts['best_hyperparameters'][-1].uri))\n",
    "    tuner_run_id = tuner_artifacts['best_hyperparameters'][-1].id\n",
    "    print(f'Generated tuner result: {tuner_run_path}')\n",
    "    print(f'Tuner execution id: {tuner_run_id}')\n",
    "except NameError:\n",
    "    print('Tuner not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_run_path = os.path.abspath(os.path.join('..', model_artifacts['model_run'][-1].uri))\n",
    "    model_run_id = model_artifacts['model_run'][-1].id\n",
    "    print(f'Generated model result: {model_run_path}')\n",
    "    print(f'Model execution id: {model_run_id}')\n",
    "except NameError:\n",
    "    print('Model not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_blessed_path = os.path.abspath(os.path.join('..', model_eval_artifacts['blessing'][-1].uri))\n",
    "    model_blessed_id = model_eval_artifacts['blessing'][-1].id\n",
    "    print(f'Generated model blessing result: {model_blessed_path}')\n",
    "    print(f'Blessing execution id: {model_blessed_id}')\n",
    "except NameError:\n",
    "    print('Model blessing not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    model_eval_path = os.path.abspath(os.path.join('..', model_eval_artifacts['evaluation'][-1].uri))\n",
    "    model_eval_id = model_eval_artifacts['evaluation'][-1].id\n",
    "    print(f'Generated model evaluation result: {model_eval_path}')\n",
    "    print(f'Evaluator execution id: {model_eval_id}')\n",
    "except NameError:\n",
    "    print('Model evaluation not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training investigation\n",
    "\n",
    "The `model_run` output acts as the working directory and can be used to output non-model related output (e.g., TensorBoard logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9579a11f018896fb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9579a11f018896fb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model results to Tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {model_run_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The `blessing` output simply states if the candidate model was blessed. The artifact URI will have a `BLESSED` or `NOT_BLESSED` file depending on the result. As mentioned earlier, this first run will pass the evaluation because there is no baseline model yet.  \n",
    "\n",
    "The `evaluation` output, on the other hand, contains the evaluation logs and can be used to visualize the global metrics on the entire evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/viktor.eriksson2/Documents/github/tfx-pipeline/.venv/lib/python3.8/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:107: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/viktor.eriksson2/Documents/github/tfx-pipeline/.venv/lib/python3.8/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:107: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_ok: true\n",
      "validation_details {\n",
      "  slicing_details {\n",
      "    slicing_spec {\n",
      "    }\n",
      "    num_matching_slices: 128\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.load_eval_result(model_eval_path)\n",
    "\n",
    "print(tfma.load_validation_result(model_eval_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering Metrics\n",
    "\n",
    "You can view the metrics with the [`tfma.view.render_slicing_metrics()`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_slicing_metrics) method. By default, the views will display the `Overall` slice. To view a particular slice you can pass in a feature name to the `slicing_column` argument as shown below. You can visualize the different metrics through the `Show` dropdown menu and you can hover over the bar charts to show the exact value measured. \n",
    "\n",
    "We encourage you to try the different options you see and also modify the command. Here are some examples:\n",
    "\n",
    "* Removing the `slicing_column` argument will produce the overall slice.\n",
    "* You can also pass in `race` (since it was specified in the eval config) to see the results for that particular slice.\n",
    "* Using the `Examples (Weighted) Threshold` slider above 5421 will remove the `Female` slice because it has less examples than that.\n",
    "* Toggling the `View` dropdown to `Metrics Histogram` will show the results divided into buckets. For example, if you're slicing column is `sex` and the `Histogram Type` dropdown is at `Slice Counts`, then you will one slice in two of the 10 (default) buckets since we only have two values for that feature ('Male' and 'Female'). The x-axis show the values for the metric in the `Select Metric` dropdown. This is the default view when the number of slices is large.\n",
    "* At the bottom of the screen, you will notice the measurements also presented in tabular format. You can sort it by clicking on the feature name headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fcdc91f1084b6db7140107c73f53d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'Overall', 'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render overall results\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More slices\n",
    "You can pass columns to slice the data. This is useful if you just want to study a subgroup of a particular feature and not the entire domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efe92a92c114a8299ee5f79dbd063ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'trip_start_month:5',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Slice on column\n",
    "tfma.view.render_slicing_metrics(eval_result,\n",
    "                                 slicing_column='trip_start_month')\n",
    "                                 # slicing_spec=tfma.slicer.SingleSliceSpec(columns=['trip_start_hour']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also slice on feature crosses to analyze combinations of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f029d091c0294627b475bce3ee291467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'trip_start_day_X_tri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Slice on feature crosses\n",
    "tfma.view.render_slicing_metrics(\n",
    "    eval_result,\n",
    "    slicing_spec=tfma.SlicingSpec(\n",
    "        feature_keys=['trip_start_day', 'trip_start_month']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crossing the two columns generate a lot of combinations! Let's narrow down our cross..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf86ec59c1c4f16b58a5d077cc8fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'trip_start_day_X_tri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(\n",
    "    eval_result,\n",
    "    slicing_spec=tfma.SlicingSpec(\n",
    "        feature_keys=['trip_start_day'], feature_values={'trip_start_month': '9'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering plots\n",
    "Any plots that were added to the `tfma.EvalConfig` as post training `metric_specs` can be displayed using [`tfma.view.render_plot`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_plot).\n",
    "\n",
    "As with metrics, plots can be viewed by slice. Unlike metrics, only plots for a particular slice value can be displayed so the `tfma.SlicingSpec` must be used and it must specify both a slice feature name and value. If no slice is provided then the plots for the `Overall` slice is used.\n",
    "\n",
    "You can click on the names at the bottom of the graph to see a different plot type. Alternatively, you can tick the `Show all plots` checkbox to show all the plots in one screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d90d35a38b468fbd373c21274840e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PlotViewer(config={'sliceName': 'Overall', 'metricKeys': {'calibrationPlot': {'metricName': 'calibrationHistog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render overall plot\n",
    "tfma.view.render_plot(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bf0e6cb5394f5e8b2042bb0b3b7859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PlotViewer(config={'sliceName': 'trip_start_hour:1', 'metricKeys': {'calibrationPlot': {'metricName': 'calibra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render plot on column value\n",
    "tfma.view.render_plot(\n",
    "    eval_result,\n",
    "    tfma.SlicingSpec(feature_values={'trip_start_hour': '1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness metrics\n",
    "\n",
    "Now you can view the fairness metrics. Try to explore the widget and see if you can make other findings. Here are some suggestions:\n",
    "\n",
    "* Change the baseline so the percentage difference (in the table below the chart) will be measured against it.\n",
    "\n",
    "* Deselect the `Overall` slice so you can compare groups side by side.\n",
    "\n",
    "* Select other metrics to display and observe their charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd1bedbd0f849669c2efd7b4e6ad846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FairnessIndicatorViewer(slicingMetrics=[{'sliceValue': '5', 'slice': 'trip_start_month:5', 'metrics': {'binary…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the fairness metrics\n",
    "tfma.addons.fairness.view.widget_view.render_fairness_indicator(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the What-if Tool to interpret your model\n",
    "\n",
    "Once your model has deployed, you're ready to connect it to the What-if Tool using the `WitWidget`.  \n",
    ">**Note**: You can ignore the message `TypeError(unsupported operand type(s) for -: 'int' and 'list')` while creating a What-if Tool visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format a subset of the test data to send to the What-if Tool for visualization\n",
    "# Append ground truth label value to training data\n",
    "\n",
    "# This is the number of examples you want to display in the What-if Tool\n",
    "num_wit_examples = 500\n",
    "test_examples = np.hstack((x_test[:num_wit_examples].values,y_test[:num_wit_examples].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a What-if Tool visualization, it may take a minute to load\n",
    "# See the cell below this for exploration ideas\n",
    "\n",
    "# # This prediction adjustment function is needed as this xgboost model's\n",
    "# # prediction returns just a score for the positive class of the binary\n",
    "# # classification, whereas the What-If Tool expects a list of scores for each\n",
    "# # class (in this case, both the negative class and the positive class).\n",
    "# def adjust_prediction(pred):\n",
    "#   return [1 - pred, pred]\n",
    "# \n",
    "# config_builder = (WitConfigBuilder(test_examples.tolist(), data.columns.tolist() + ['mortgage_status'])\n",
    "#   .set_ai_platform_model(GCP_PROJECT, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n",
    "#   .set_target_feature('mortgage_status')\n",
    "#   .set_label_vocab(['denied', 'approved']))\n",
    "# WitWidget(config_builder, height=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
