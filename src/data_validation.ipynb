{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data validation\n",
    "\n",
    "This notebook briefly shows some ways and techniques for analysing artifacts from Generator components (e.g. ExampleGen, SchemaGen, etc.). All artifacts are fetched from the metadata storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libs\n",
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_data_validation.utils.anomalies_util import load_anomalies_binary\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.types import standard_artifacts, standard_component_specs\n",
    "from tfx.orchestration.experimental.interactive import visualizations, standard_visualizations\n",
    "\n",
    "from pipeline.configs import PIPELINE_NAME\n",
    "\n",
    "from utils.mlmd_helpers import get_latest_artifacts, visualize_artifacts_nb\n",
    "from utils.tfx_helpers import get_records\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "standard_visualizations.register_standard_visualizations()\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "print(f'TF version: {tf.version.VERSION}')\n",
    "print(f'TFDV version: {tfdv.version.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Metadata artifacts\n",
    "\n",
    "In order to investigate generated components from the pipeline we need to fetch the desired artifacts.  \n",
    "\n",
    "We start by fetching the artifacts (if generated) from `ExampleGen`, `StatisticsGen`, `SchemaGen`, `ExampleValidator`, and `Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read artifact information from metadata store.\n",
    "\n",
    "# Metadata store path\n",
    "METADATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..',\n",
    "                                             'outputs/tfx_metadata',\n",
    "                                             PIPELINE_NAME,\n",
    "                                             'metadata.db'))\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..', 'data/chicago_taxi_trips/data.csv'))\n",
    "\n",
    "# Metadata store config\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "              METADATA_PATH)\n",
    "\n",
    "with metadata.Metadata(metadata_connection_config) as store:\n",
    "    # Load ExampleGen artifacts (generated before transform)\n",
    "    try:\n",
    "        example_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'CsvExampleGen')\n",
    "        # example_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'FileBasedExampleGen')\n",
    "    except AttributeError:\n",
    "        print('CsvExampleGen not available')\n",
    "    \n",
    "    # Load StatisticsGen artifacts\n",
    "    try:\n",
    "        # stats_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleStatistics.TYPE_NAME)\n",
    "        stats_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'StatisticsGen')\n",
    "    except AttributeError:\n",
    "        print('StatisticsGen not available')\n",
    "    \n",
    "    # Load SchemaGen artifacts\n",
    "    try:\n",
    "        # schema_artifacts = store.get_artifacts_by_type(standard_artifacts.Schema.TYPE_NAME)\n",
    "        schema_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'SchemaGen')\n",
    "    except AttributeError:\n",
    "        print('SchemaGen not available')\n",
    "    \n",
    "    # Load ExampleValidator artifacts\n",
    "    try:\n",
    "        # anomalies_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleAnomalies.TYPE_NAME)\n",
    "        anomalies_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'ExampleValidator')\n",
    "    except AttributeError:\n",
    "        print('ExampleValidator not available')\n",
    "\n",
    "    # Load Transform artifacts\n",
    "    try:\n",
    "        transform_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Transform')\n",
    "    except AttributeError:\n",
    "        print('Transform not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the URI's of the arifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract artifacts URI paths and execution IDs\n",
    "try:\n",
    "    example_path = os.path.abspath(os.path.join('..', example_artifacts['examples'][-1].uri))\n",
    "    example_id = example_artifacts['examples'][-1].id\n",
    "    train_uri = os.path.join(example_path, 'Split-train')\n",
    "    print(f'Training data example URI: {train_uri}')\n",
    "    print(f'Training data execution id: {example_id}')\n",
    "except NameError:\n",
    "    print('Examples not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    stats_path = os.path.abspath(os.path.join('..', stats_artifacts['statistics'][-1].uri))\n",
    "    stats_id = stats_artifacts['statistics'][-1].id\n",
    "    train_stats_file = os.path.join(stats_path, 'Split-train', 'FeatureStats.pb')\n",
    "    eval_stats_file = os.path.join(stats_path, 'Split-eval', 'FeatureStats.pb')\n",
    "    print(f'Train stats file: {train_stats_file}, \\nEval stats file: {eval_stats_file}')\n",
    "    print(f'Statistics execution id: {stats_id}')\n",
    "except NameError:\n",
    "    print('Statistics not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    schema_path = os.path.abspath(os.path.join('..', schema_artifacts['schema'][-1].uri))\n",
    "    schema_id = schema_artifacts['schema'][-1].id\n",
    "    schema_file = os.path.join(schema_path, 'schema.pbtxt')\n",
    "    print(f'Generated schema file: {schema_file}')\n",
    "    print(f'Schema execution id: {schema_id}')\n",
    "except NameError:\n",
    "    print('Schema not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    anomalies_path = os.path.abspath(os.path.join('..', anomalies_artifacts['anomalies'][-1].uri))\n",
    "    anomalies_id = anomalies_artifacts['anomalies'][-1].id\n",
    "    anomalies_file = os.path.join(anomalies_path, 'Split-eval', 'SchemaDiff.pb')\n",
    "    print(f'Generated anomalies file: {anomalies_file}')\n",
    "    print(f'Anomalies execution id: {anomalies_id}')\n",
    "except NameError:\n",
    "    print('Anomalies not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    tf_examples_path = os.path.abspath(os.path.join('..', transform_artifacts['transformed_examples'][-1].uri))\n",
    "    tf_examples_id = transform_artifacts['transformed_examples'][-1].id\n",
    "    tf_examples_uri = os.path.join(tf_examples_path, 'Split-train')\n",
    "    \n",
    "    tf_stats_path = os.path.abspath(os.path.join('..', transform_artifacts['post_transform_stats'][-1].uri))\n",
    "    tf_stats_id = transform_artifacts['post_transform_stats'][-1].id\n",
    "    tf_stats_file = os.path.join(tf_stats_path, 'FeatureStats.pb')\n",
    "\n",
    "    tf_anom_path = os.path.abspath(os.path.join('..', transform_artifacts['post_transform_anomalies'][-1].uri))\n",
    "    tf_anom_id = transform_artifacts['post_transform_anomalies'][-1].id\n",
    "    tf_anom_file = os.path.join(tf_anom_path, 'SchemaDiff.pb')\n",
    "\n",
    "    print(f'Transformed training data example URI: {tf_examples_uri}')\n",
    "    print(f'Transformed training data execution id: {tf_examples_id}')\n",
    "    print(f'Generated post-transform stats file: {tf_stats_file}')\n",
    "    print(f'Transform stats execution id: {tf_stats_id}')\n",
    "    print(f'Generated post-transform anomalies file: {tf_anom_file}')\n",
    "    print(f'Transform anomalies execution id: {tf_anom_id}')\n",
    "    \n",
    "except NameError:\n",
    "    print('Transform not defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation\n",
    "\n",
    "Next steps is to visualize the data.  \n",
    "\n",
    "We start by viewing the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preview the first few rows of the CSV file\n",
    "!head -n 5 {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preview first row as TFRecordDataset\n",
    "try:\n",
    "    data_files = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
    "\n",
    "    # Create a `TFRecordDataset` to read the file\n",
    "    dataset = tf.data.TFRecordDataset(data_files, compression_type=\"GZIP\")\n",
    "\n",
    "    # Get records from the dataset\n",
    "    sample_records = get_records(dataset=dataset, num_records=1)\n",
    "\n",
    "    # Print records\n",
    "    pp.pprint(sample_records)\n",
    "except NameError:\n",
    "    print('train_uri not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# It can also be nice to see the data in a pandas dataframe\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.head(10))\n",
    "print('-' * 30)\n",
    "print(f'Shape of data: {df.shape}')\n",
    "print('-' * 30)\n",
    "print(df.describe().T)\n",
    "print('-' * 30)\n",
    "print('Missing rate:\\n', df.isna().sum() / df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Data Validation\n",
    "\n",
    "We have loaded the statistics of both our train and evals sets. These will now be visualized and compared using the `tensorflow_data_validation` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated statistics from StatisticsGen\n",
    "try:\n",
    "    train_stats = tfdv.load_stats_binary(train_stats_file)\n",
    "    eval_stats = tfdv.load_stats_binary(eval_stats_file)\n",
    "    tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
    "                              lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n",
    "except NameError:\n",
    "    print('train_stats/eval_stats not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We review the schema created from the statistics.\n",
    "\n",
    "> **Note**: that the schema is based on the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated schema from SchemaGen\n",
    "try:\n",
    "    schema = tfdv.load_schema_text(schema_file)\n",
    "    tfdv.display_schema(schema=schema)\n",
    "except NameError:\n",
    "    print('schema not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we review if there are any anomalies detected in the `eval` dataset. The anomalies are calculated based on the generated statistics and schema from the `train` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data vaildation result from ExampleValidator\n",
    "try:\n",
    "    anomalies = load_anomalies_binary(anomalies_file)\n",
    "    tfdv.display_anomalies(anomalies)\n",
    "except NameError:\n",
    "    print('anomalies not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we apply some transformations to the data before training a model it can be of interest to review that data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preview first transformed data\n",
    "try:\n",
    "    tf_data_files = [os.path.join(tf_examples_uri, name) for name in os.listdir(tf_examples_uri)]\n",
    "\n",
    "    # Create a `TFRecordDataset` to read the file\n",
    "    tf_dataset = tf.data.TFRecordDataset(tf_data_files, compression_type=\"GZIP\")\n",
    "\n",
    "    # Get records from the dataset\n",
    "    tf_sample_records = get_records(dataset=tf_dataset, num_records=3)\n",
    "\n",
    "    # Print records\n",
    "    pp.pprint(tf_sample_records)\n",
    "except NameError:\n",
    "    print('tf_examples_uri not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the statistics visualization of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated statistics from Transform\n",
    "try:\n",
    "    tf_stats = tfdv.load_stats_binary(tf_stats_file)\n",
    "    tfdv.visualize_statistics(tf_stats)\n",
    "except NameError:\n",
    "    print('post-transform stats not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check if there are any anomalies detected in the transformed eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data vaildation result from Transform\n",
    "try:\n",
    "    tf_anomalies = load_anomalies_binary(tf_anom_file)\n",
    "    tfdv.display_anomalies(tf_anomalies)\n",
    "except NameError:\n",
    "    print('post-transform anomalies not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Wrap up\n",
    "\n",
    "And we're done! You have now investigated the artifacts generated by the data related components of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
