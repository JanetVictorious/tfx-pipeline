{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data validation\n",
    "\n",
    "This notebook briefly shows some ways and techniques for analysing artifacts from Generator components (e.g. ExampleGen, SchemaGen, etc.). All artifacts are fetched from the metadata storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.7.0\n",
      "TFDV version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libs\n",
    "import glob\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_data_validation.utils.anomalies_util import load_anomalies_binary\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.types import standard_artifacts, standard_component_specs\n",
    "from tfx.orchestration.experimental.interactive import visualizations, standard_visualizations\n",
    "\n",
    "from pipeline.configs import PIPELINE_NAME\n",
    "\n",
    "from utils.mlmd_helpers import get_latest_artifacts, visualize_artifacts_nb\n",
    "from utils.tfx_helpers import get_records\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "standard_visualizations.register_standard_visualizations()\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "print(f'TF version: {tf.version.VERSION}')\n",
    "print(f'TFDV version: {tfdv.version.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Metadata artifacts\n",
    "\n",
    "In order to investigate generated components from the pipeline we need to fetch the desired artifacts.  \n",
    "\n",
    "We start by fetching the artifacts (if generated) from `ExampleGen`, `StatisticsGen`, `SchemaGen`, `ExampleValidator`, and `Transformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read artifact information from metadata store.\n",
    "\n",
    "# Metadata store path\n",
    "METADATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..',\n",
    "                                             'outputs/tfx_metadata',\n",
    "                                             PIPELINE_NAME,\n",
    "                                             'metadata.db'))\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = os.path.abspath(os.path.join(os.getcwd(), '..', 'data/chicago_taxi_trips/data.csv'))\n",
    "\n",
    "# Metadata store config\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "              METADATA_PATH)\n",
    "\n",
    "with metadata.Metadata(metadata_connection_config) as store:\n",
    "    # Load ExampleGen artifacts (generated before transform)\n",
    "    try:\n",
    "        example_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'CsvExampleGen')\n",
    "        # example_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'FileBasedExampleGen')\n",
    "    except AttributeError:\n",
    "        print('CsvExampleGen not available')\n",
    "    \n",
    "    # Load StatisticsGen artifacts\n",
    "    try:\n",
    "        # stats_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleStatistics.TYPE_NAME)\n",
    "        stats_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'StatisticsGen')\n",
    "    except AttributeError:\n",
    "        print('StatisticsGen not available')\n",
    "    \n",
    "    # Load SchemaGen artifacts\n",
    "    try:\n",
    "        # schema_artifacts = store.get_artifacts_by_type(standard_artifacts.Schema.TYPE_NAME)\n",
    "        schema_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'SchemaGen')\n",
    "    except AttributeError:\n",
    "        print('SchemaGen not available')\n",
    "    \n",
    "    # Load ExampleValidator artifacts\n",
    "    try:\n",
    "        # anomalies_artifacts = store.get_artifacts_by_type(standard_artifacts.ExampleAnomalies.TYPE_NAME)\n",
    "        anomalies_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'ExampleValidator')\n",
    "    except AttributeError:\n",
    "        print('ExampleValidator not available')\n",
    "\n",
    "    # Load Transform artifacts\n",
    "    try:\n",
    "        transform_artifacts = get_latest_artifacts(store, PIPELINE_NAME, 'Transform')\n",
    "    except AttributeError:\n",
    "        print('Transform not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the URI's of the arifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data example URI: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/CsvExampleGen/examples/1/Split-train\n",
      "Training data execution id: 1\n",
      "----------\n",
      "Train stats file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/StatisticsGen/statistics/3/Split-train/FeatureStats.pb, \n",
      "Eval stats file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/StatisticsGen/statistics/3/Split-eval/FeatureStats.pb\n",
      "Statistics execution id: 2\n",
      "----------\n",
      "Generated schema file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/SchemaGen/schema/4/schema.pbtxt\n",
      "Schema execution id: 3\n",
      "----------\n",
      "Generated anomalies file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/ExampleValidator/anomalies/5/Split-eval/SchemaDiff.pb\n",
      "Anomalies execution id: 4\n",
      "----------\n",
      "Transformed training data example URI: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Transform/transformed_examples/6/Split-train\n",
      "Transformed training data execution id: 10\n",
      "Generated post-transform stats file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Transform/post_transform_stats/6/FeatureStats.pb\n",
      "Transform stats execution id: 5\n",
      "Generated post-transform anomalies file: /Users/viktor.eriksson2/Documents/github/tfx-pipeline/outputs/tfx_pipeline_output/taxi_pipeline/Transform/post_transform_anomalies/6/SchemaDiff.pb\n",
      "Transform anomalies execution id: 12\n"
     ]
    }
   ],
   "source": [
    "# Extract artifacts URI paths and execution IDs\n",
    "try:\n",
    "    example_path = os.path.abspath(os.path.join('..', example_artifacts['examples'][-1].uri))\n",
    "    example_id = example_artifacts['examples'][-1].id\n",
    "    train_uri = os.path.join(example_path, 'Split-train')\n",
    "    print(f'Training data example URI: {train_uri}')\n",
    "    print(f'Training data execution id: {example_id}')\n",
    "except NameError:\n",
    "    print('Examples not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    stats_path = os.path.abspath(os.path.join('..', stats_artifacts['statistics'][-1].uri))\n",
    "    stats_id = stats_artifacts['statistics'][-1].id\n",
    "    train_stats_file = os.path.join(stats_path, 'Split-train', 'FeatureStats.pb')\n",
    "    eval_stats_file = os.path.join(stats_path, 'Split-eval', 'FeatureStats.pb')\n",
    "    print(f'Train stats file: {train_stats_file}, \\nEval stats file: {eval_stats_file}')\n",
    "    print(f'Statistics execution id: {stats_id}')\n",
    "except NameError:\n",
    "    print('Statistics not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    schema_path = os.path.abspath(os.path.join('..', schema_artifacts['schema'][-1].uri))\n",
    "    schema_id = schema_artifacts['schema'][-1].id\n",
    "    schema_file = os.path.join(schema_path, 'schema.pbtxt')\n",
    "    print(f'Generated schema file: {schema_file}')\n",
    "    print(f'Schema execution id: {schema_id}')\n",
    "except NameError:\n",
    "    print('Schema not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    anomalies_path = os.path.abspath(os.path.join('..', anomalies_artifacts['anomalies'][-1].uri))\n",
    "    anomalies_id = anomalies_artifacts['anomalies'][-1].id\n",
    "    anomalies_file = os.path.join(anomalies_path, 'Split-eval', 'SchemaDiff.pb')\n",
    "    print(f'Generated anomalies file: {anomalies_file}')\n",
    "    print(f'Anomalies execution id: {anomalies_id}')\n",
    "except NameError:\n",
    "    print('Anomalies not defined')\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "try:\n",
    "    tf_examples_path = os.path.abspath(os.path.join('..', transform_artifacts['transformed_examples'][-1].uri))\n",
    "    tf_examples_id = transform_artifacts['transformed_examples'][-1].id\n",
    "    tf_examples_uri = os.path.join(tf_examples_path, 'Split-train')\n",
    "    \n",
    "    tf_stats_path = os.path.abspath(os.path.join('..', transform_artifacts['post_transform_stats'][-1].uri))\n",
    "    tf_stats_id = transform_artifacts['post_transform_stats'][-1].id\n",
    "    tf_stats_file = os.path.join(tf_stats_path, 'FeatureStats.pb')\n",
    "\n",
    "    tf_anom_path = os.path.abspath(os.path.join('..', transform_artifacts['post_transform_anomalies'][-1].uri))\n",
    "    tf_anom_id = transform_artifacts['post_transform_anomalies'][-1].id\n",
    "    tf_anom_file = os.path.join(tf_anom_path, 'SchemaDiff.pb')\n",
    "\n",
    "    print(f'Transformed training data example URI: {tf_examples_uri}')\n",
    "    print(f'Transformed training data execution id: {tf_examples_id}')\n",
    "    print(f'Generated post-transform stats file: {tf_stats_file}')\n",
    "    print(f'Transform stats execution id: {tf_stats_id}')\n",
    "    print(f'Generated post-transform anomalies file: {tf_anom_file}')\n",
    "    print(f'Transform anomalies execution id: {tf_anom_id}')\n",
    "    \n",
    "except NameError:\n",
    "    print('Transform not defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation\n",
    "\n",
    "Next steps is to visualize the data.  \n",
    "\n",
    "We start by viewing the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_community_area,fare,trip_start_month,trip_start_hour,trip_start_day,trip_start_timestamp,pickup_latitude,pickup_longitude,dropoff_latitude,dropoff_longitude,trip_miles,pickup_census_tract,dropoff_census_tract,payment_type,company,trip_seconds,dropoff_community_area,tips\r\n",
      ",12.45,5,19,6,1400269500,,,,,0.0,,,Credit Card,Chicago Elite Cab Corp. (Chicago Carriag,0,,0.0\r\n",
      ",0,3,19,5,1362683700,,,,,0,,,Unknown,Chicago Elite Cab Corp.,300,,0\r\n",
      "60,27.05,10,2,3,1380593700,41.836150155,-87.648787952,,,12.6,,,Cash,Taxi Affiliation Services,1380,,0.0\r\n",
      "10,5.85,10,1,2,1382319000,41.985015101,-87.804532006,,,0.0,,,Cash,Taxi Affiliation Services,180,,0.0\r\n",
      "14,16.65,5,7,5,1369897200,41.968069,-87.721559063,,,0.0,,,Cash,Dispatch Taxi Affiliation,1080,,0.0\r\n"
     ]
    }
   ],
   "source": [
    "# Preview the first few rows of the CSV file\n",
    "!head -n 6 {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features': {'feature': {'company': {'bytesList': {'value': ['Q2hpY2FnbyBFbGl0ZSBDYWIgQ29ycC4gKENoaWNhZ28gQ2FycmlhZw==']}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [12.45]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['Q3JlZGl0IENhcmQ=']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {}},\n",
      "                           'pickup_latitude': {'floatList': {}},\n",
      "                           'pickup_longitude': {'floatList': {}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['0']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['6']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['19']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['5']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1400269500']}}}}},\n",
      " {'features': {'feature': {'company': {'bytesList': {'value': ['Q2hpY2FnbyBFbGl0ZSBDYWIgQ29ycC4=']}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [0.0]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['VW5rbm93bg==']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {}},\n",
      "                           'pickup_latitude': {'floatList': {}},\n",
      "                           'pickup_longitude': {'floatList': {}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['300']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['5']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['19']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['3']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1362683700']}}}}},\n",
      " {'features': {'feature': {'company': {'bytesList': {'value': ['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [27.05]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['Q2FzaA==']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {'value': ['60']}},\n",
      "                           'pickup_latitude': {'floatList': {'value': [41.83615]}},\n",
      "                           'pickup_longitude': {'floatList': {'value': [-87.64879]}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [12.6]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['1380']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['3']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['2']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['10']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1380593700']}}}}},\n",
      " {'features': {'feature': {'company': {'bytesList': {'value': ['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [5.85]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['Q2FzaA==']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {'value': ['10']}},\n",
      "                           'pickup_latitude': {'floatList': {'value': [41.985016]}},\n",
      "                           'pickup_longitude': {'floatList': {'value': [-87.804535]}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['180']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['2']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['1']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['10']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1382319000']}}}}},\n",
      " {'features': {'feature': {'company': {'bytesList': {}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [16.45]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['Q2FzaA==']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {'value': ['13']}},\n",
      "                           'pickup_latitude': {'floatList': {'value': [41.983635]}},\n",
      "                           'pickup_longitude': {'floatList': {'value': [-87.72358]}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [6.9]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['780']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['3']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['12']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['11']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1446554700']}}}}},\n",
      " {'features': {'feature': {'company': {'bytesList': {}},\n",
      "                           'dropoff_census_tract': {'int64List': {}},\n",
      "                           'dropoff_community_area': {'int64List': {}},\n",
      "                           'dropoff_latitude': {'floatList': {}},\n",
      "                           'dropoff_longitude': {'floatList': {}},\n",
      "                           'fare': {'floatList': {'value': [32.05]}},\n",
      "                           'payment_type': {'bytesList': {'value': ['Q2FzaA==']}},\n",
      "                           'pickup_census_tract': {'int64List': {}},\n",
      "                           'pickup_community_area': {'int64List': {'value': ['16']}},\n",
      "                           'pickup_latitude': {'floatList': {'value': [41.953583]}},\n",
      "                           'pickup_longitude': {'floatList': {'value': [-87.72345]}},\n",
      "                           'tips': {'floatList': {'value': [0.0]}},\n",
      "                           'trip_miles': {'floatList': {'value': [15.4]}},\n",
      "                           'trip_seconds': {'int64List': {'value': ['1200']}},\n",
      "                           'trip_start_day': {'int64List': {'value': ['1']}},\n",
      "                           'trip_start_hour': {'int64List': {'value': ['1']}},\n",
      "                           'trip_start_month': {'int64List': {'value': ['12']}},\n",
      "                           'trip_start_timestamp': {'int64List': {'value': ['1417916700']}}}}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 13:17:37.621051: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Preview first row(s) as TFRecordDataset\n",
    "try:\n",
    "    data_files = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
    "\n",
    "    # Create a `TFRecordDataset` to read the file\n",
    "    dataset = tf.data.TFRecordDataset(data_files, compression_type=\"GZIP\")\n",
    "\n",
    "    # Get records from the dataset\n",
    "    sample_records = get_records(dataset=dataset, num_records=6)\n",
    "\n",
    "    # Print records\n",
    "    pp.pprint(sample_records)\n",
    "except NameError:\n",
    "    print('train_uri not defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pickup_community_area   fare  trip_start_month  trip_start_hour  \\\n",
      "0                    NaN  12.45                 5               19   \n",
      "1                    NaN   0.00                 3               19   \n",
      "2                   60.0  27.05                10                2   \n",
      "3                   10.0   5.85                10                1   \n",
      "4                   14.0  16.65                 5                7   \n",
      "5                   13.0  16.45                11               12   \n",
      "6                   16.0  32.05                12                1   \n",
      "7                   30.0  38.45                10               10   \n",
      "8                   11.0  14.65                 1                1   \n",
      "9                   33.0   3.25                 5               17   \n",
      "\n",
      "   trip_start_day  trip_start_timestamp  pickup_latitude  pickup_longitude  \\\n",
      "0               6            1400269500              NaN               NaN   \n",
      "1               5            1362683700              NaN               NaN   \n",
      "2               3            1380593700        41.836150        -87.648788   \n",
      "3               2            1382319000        41.985015        -87.804532   \n",
      "4               5            1369897200        41.968069        -87.721559   \n",
      "5               3            1446554700        41.983636        -87.723583   \n",
      "6               1            1417916700        41.953582        -87.723452   \n",
      "7               5            1444301100        41.839087        -87.714004   \n",
      "8               3            1358213400        41.978830        -87.771167   \n",
      "9               1            1368985500        41.849247        -87.624135   \n",
      "\n",
      "   dropoff_latitude  dropoff_longitude  trip_miles  pickup_census_tract  \\\n",
      "0               NaN                NaN        0.00                  NaN   \n",
      "1               NaN                NaN        0.00                  NaN   \n",
      "2               NaN                NaN       12.60                  NaN   \n",
      "3               NaN                NaN        0.00                  NaN   \n",
      "4               NaN                NaN        0.00                  NaN   \n",
      "5               NaN                NaN        6.90                  NaN   \n",
      "6               NaN                NaN       15.40                  NaN   \n",
      "7               NaN                NaN       14.60                  NaN   \n",
      "8               NaN                NaN        5.81                  NaN   \n",
      "9               NaN                NaN        0.00                  NaN   \n",
      "\n",
      "   dropoff_census_tract payment_type  \\\n",
      "0                   NaN  Credit Card   \n",
      "1                   NaN      Unknown   \n",
      "2                   NaN         Cash   \n",
      "3                   NaN         Cash   \n",
      "4                   NaN         Cash   \n",
      "5                   NaN         Cash   \n",
      "6                   NaN         Cash   \n",
      "7                   NaN         Cash   \n",
      "8                   NaN         Cash   \n",
      "9                   NaN         Cash   \n",
      "\n",
      "                                    company  trip_seconds  \\\n",
      "0  Chicago Elite Cab Corp. (Chicago Carriag           0.0   \n",
      "1                   Chicago Elite Cab Corp.         300.0   \n",
      "2                 Taxi Affiliation Services        1380.0   \n",
      "3                 Taxi Affiliation Services         180.0   \n",
      "4                 Dispatch Taxi Affiliation        1080.0   \n",
      "5                                       NaN         780.0   \n",
      "6                                       NaN        1200.0   \n",
      "7                                       NaN        2580.0   \n",
      "8                                       NaN        1080.0   \n",
      "9                 Taxi Affiliation Services           0.0   \n",
      "\n",
      "   dropoff_community_area  tips  \n",
      "0                     NaN   0.0  \n",
      "1                     NaN   0.0  \n",
      "2                     NaN   0.0  \n",
      "3                     NaN   0.0  \n",
      "4                     NaN   0.0  \n",
      "5                     NaN   0.0  \n",
      "6                     NaN   0.0  \n",
      "7                     NaN   0.0  \n",
      "8                     NaN   0.0  \n",
      "9                     NaN   0.0  \n",
      "------------------------------\n",
      "Shape of data: (15002, 18)\n",
      "------------------------------\n",
      "                          count          mean           std           min  \\\n",
      "pickup_community_area   15000.0  2.225027e+01  1.941483e+01  1.000000e+00   \n",
      "fare                    15002.0  1.176822e+01  1.153885e+01  0.000000e+00   \n",
      "trip_start_month        15002.0  6.585655e+00  3.390997e+00  1.000000e+00   \n",
      "trip_start_hour         15002.0  1.363232e+01  6.620927e+00  0.000000e+00   \n",
      "trip_start_day          15002.0  4.186642e+00  2.015694e+00  1.000000e+00   \n",
      "trip_start_timestamp    15002.0  1.408495e+09  2.916043e+07  1.357000e+09   \n",
      "pickup_latitude         15000.0  4.190305e+01  3.775149e-02  4.169488e+01   \n",
      "pickup_longitude        15000.0 -8.765755e+01  6.784619e-02 -8.791362e+01   \n",
      "dropoff_latitude        14519.0  4.190267e+01  3.847781e-02  4.166367e+01   \n",
      "dropoff_longitude       14519.0 -8.765411e+01  5.661618e-02 -8.791362e+01   \n",
      "trip_miles              15002.0  2.872820e+00  1.527601e+01  0.000000e+00   \n",
      "pickup_census_tract         1.0  1.703108e+10           NaN  1.703108e+10   \n",
      "dropoff_census_tract    10761.0  1.703135e+10  3.312243e+05  1.703101e+10   \n",
      "trip_seconds            14996.0  7.776275e+02  9.775388e+02  0.000000e+00   \n",
      "dropoff_community_area  14495.0  2.096778e+01  1.764106e+01  1.000000e+00   \n",
      "tips                    15002.0  1.076674e+00  2.158340e+00  0.000000e+00   \n",
      "\n",
      "                                 25%           50%           75%           max  \n",
      "pickup_community_area   8.000000e+00  8.000000e+00  3.200000e+01  7.700000e+01  \n",
      "fare                    5.850000e+00  7.850000e+00  1.245000e+01  7.000700e+02  \n",
      "trip_start_month        4.000000e+00  7.000000e+00  1.000000e+01  1.200000e+01  \n",
      "trip_start_hour         9.000000e+00  1.500000e+01  1.900000e+01  2.300000e+01  \n",
      "trip_start_day          2.000000e+00  4.000000e+00  6.000000e+00  7.000000e+00  \n",
      "trip_start_timestamp    1.384622e+09  1.407260e+09  1.431339e+09  1.483116e+09  \n",
      "pickup_latitude         4.188099e+01  4.189251e+01  4.192188e+01  4.200962e+01  \n",
      "pickup_longitude       -8.765600e+01 -8.763331e+01 -8.762621e+01 -8.757278e+01  \n",
      "dropoff_latitude        4.188099e+01  4.189322e+01  4.192269e+01  4.202122e+01  \n",
      "dropoff_longitude      -8.765680e+01 -8.763416e+01 -8.762621e+01 -8.754094e+01  \n",
      "trip_miles              0.000000e+00  1.000000e+00  2.500000e+00  1.710000e+03  \n",
      "pickup_census_tract     1.703108e+10  1.703108e+10  1.703108e+10  1.703108e+10  \n",
      "dropoff_census_tract    1.703108e+10  1.703124e+10  1.703183e+10  1.703198e+10  \n",
      "trip_seconds            3.600000e+02  5.400000e+02  9.600000e+02  7.212000e+04  \n",
      "dropoff_community_area  8.000000e+00  1.200000e+01  3.200000e+01  7.700000e+01  \n",
      "tips                    0.000000e+00  0.000000e+00  2.000000e+00  4.700000e+01  \n",
      "------------------------------\n",
      "Missing rate:\n",
      " pickup_community_area     0.000133\n",
      "fare                      0.000000\n",
      "trip_start_month          0.000000\n",
      "trip_start_hour           0.000000\n",
      "trip_start_day            0.000000\n",
      "trip_start_timestamp      0.000000\n",
      "pickup_latitude           0.000133\n",
      "pickup_longitude          0.000133\n",
      "dropoff_latitude          0.032196\n",
      "dropoff_longitude         0.032196\n",
      "trip_miles                0.000000\n",
      "pickup_census_tract       0.999933\n",
      "dropoff_census_tract      0.282696\n",
      "payment_type              0.000000\n",
      "company                   0.342621\n",
      "trip_seconds              0.000400\n",
      "dropoff_community_area    0.033795\n",
      "tips                      0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# It can also be nice to see the data in a pandas dataframe\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.head(10))\n",
    "print('-' * 30)\n",
    "print(f'Shape of data: {df.shape}')\n",
    "print('-' * 30)\n",
    "print(df.describe().T)\n",
    "print('-' * 30)\n",
    "print('Missing rate:\\n', df.isna().sum() / df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Data Validation\n",
    "\n",
    "We have loaded the statistics of both our train and evals sets. These will now be visualized and compared using the `tensorflow_data_validation` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated statistics from StatisticsGen\n",
    "try:\n",
    "    train_stats = tfdv.load_stats_binary(train_stats_file)\n",
    "    eval_stats = tfdv.load_stats_binary(eval_stats_file)\n",
    "    tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
    "                              lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n",
    "except NameError:\n",
    "    print('train_stats/eval_stats not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We review the schema created from the statistics.\n",
    "\n",
    "> **Note**: that the schema is based on the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated schema from SchemaGen\n",
    "try:\n",
    "    schema = tfdv.load_schema_text(schema_file)\n",
    "    tfdv.display_schema(schema=schema)\n",
    "except NameError:\n",
    "    print('schema not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we review if there are any anomalies detected in the `eval` dataset. The anomalies are calculated based on the generated statistics and schema from the `train` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data vaildation result from ExampleValidator\n",
    "try:\n",
    "    anomalies = load_anomalies_binary(anomalies_file)\n",
    "    tfdv.display_anomalies(anomalies)\n",
    "except NameError:\n",
    "    print('anomalies not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we apply some transformations to the data before training a model it can be of interest to review that data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preview first transformed data\n",
    "try:\n",
    "    tf_data_files = [os.path.join(tf_examples_uri, name) for name in os.listdir(tf_examples_uri)]\n",
    "\n",
    "    # Create a `TFRecordDataset` to read the file\n",
    "    tf_dataset = tf.data.TFRecordDataset(tf_data_files, compression_type=\"GZIP\")\n",
    "\n",
    "    # Get records from the dataset\n",
    "    tf_sample_records = get_records(dataset=tf_dataset, num_records=6)\n",
    "\n",
    "    # Print records\n",
    "    pp.pprint(tf_sample_records)\n",
    "except NameError:\n",
    "    print('tf_examples_uri not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the statistics visualization of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load generated statistics from Transform\n",
    "try:\n",
    "    tf_stats = tfdv.load_stats_binary(tf_stats_file)\n",
    "    tfdv.visualize_statistics(tf_stats)\n",
    "except NameError:\n",
    "    print('post-transform stats not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check if there are any anomalies detected in the transformed eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load data vaildation result from Transform\n",
    "try:\n",
    "    tf_anomalies = load_anomalies_binary(tf_anom_file)\n",
    "    tfdv.display_anomalies(tf_anomalies)\n",
    "except NameError:\n",
    "    print('post-transform anomalies not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Wrap up\n",
    "\n",
    "And we're done! You have now investigated the artifacts generated by the data related components of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
